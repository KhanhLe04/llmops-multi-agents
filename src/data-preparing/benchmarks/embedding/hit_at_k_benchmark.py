#!/usr/bin/env python3
"""
Hit@K Benchmark for Embedding Models
Test embedding models using Hit@1, Hit@4 metrics on ViSTS dataset
"""

import os
import sys
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# Import our modules
try:
    # Try to import local config first
    from config import *
except ImportError:
    # Fallback to parent directory config
    sys.path.append('../../')
    from config import Config

class HitAtKBenchmark:
    def __init__(self):
        """
        Initialize Hit@K Benchmark vá»›i ViSTS dataset
        """
        print("ğŸ¯ Khá»Ÿi táº¡o Hit@K Benchmark...")
        # Use models from config
        self.models_to_test = HIT_AT_K_MODELS
        self.results = {}
        self.dataset = None
        
    def load_vists_dataset(self) -> Dict:
        """
        Load ViSTS dataset tá»« HuggingFace
        """
        print("ğŸ“š Loading ViSTS dataset...")
        
        try:
            # Load all subsets cá»§a ViSTS
            try:
                datasets = VISTS_DATASETS
            except NameError:
                datasets = ['STS-B', 'STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STS-Sickr']
            all_data = []
            
            for dataset_name in datasets:
                print(f"   Loading {dataset_name}...")
                try:
                    sts_data = load_dataset("anti-ai/ViSTS", dataset_name)["test"]
                    
                    for item in sts_data:
                        all_data.append({
                            'sentence1': item['sentence1'],
                            'sentence2': item['sentence2'], 
                            'score': float(item['score']) / 5.0,  # Normalize 0-1
                            'dataset': dataset_name
                        })
                        
                except Exception as e:
                    print(f"   âš ï¸  Lá»—i loading {dataset_name}: {e}")
                    continue
            
            print(f"âœ… Loaded {len(all_data)} sentence pairs from {len(datasets)} datasets")
            
            # Convert to DataFrame
            df = pd.DataFrame(all_data)
            
            # Filter high similarity pairs (score >= 0.6) for positive examples
            positive_pairs = df[df['score'] >= 0.6].copy()
            print(f"ğŸ“Š Positive pairs (score >= 0.6): {len(positive_pairs)}")
            
            self.dataset = {
                'all_data': df,
                'positive_pairs': positive_pairs,
                'total_pairs': len(all_data)
            }
            
            return self.dataset
            
        except Exception as e:
            print(f"âŒ Lá»—i loading ViSTS dataset: {e}")
            return None
    
    def load_model_safe(self, model_name: str) -> Optional[SentenceTransformer]:
        """
        Load model vá»›i error handling
        """
        try:
            print(f"   Loading {model_name}...")
            
            # Try normal loading first
            try:
                model = SentenceTransformer(model_name)
            except ValueError as ve:
                if "trust_remote_code" in str(ve):
                    print(f"   âš ï¸  Requires trust_remote_code=True, retrying...")
                    model = SentenceTransformer(model_name, trust_remote_code=True)
                else:
                    raise ve
            
            print(f"   âœ… Successfully loaded {model_name}")
            return model
            
        except Exception as e:
            print(f"   âŒ Failed to load {model_name}: {e}")
            return None
    
    def compute_hit_at_k(self, query_embeddings: np.ndarray, 
                        candidate_embeddings: np.ndarray, 
                        true_indices: List[int], 
                        k_values: List[int] = [1, 4]) -> Dict[int, float]:
        """
        Compute Hit@K metrics
        
        Args:
            query_embeddings: Query embeddings (n_queries, dim)
            candidate_embeddings: Candidate embeddings (n_candidates, dim) 
            true_indices: True positive indices for each query
            k_values: K values to compute Hit@K for
            
        Returns:
            Dict with Hit@K scores
        """
        n_queries = len(query_embeddings)
        hit_scores = {k: 0 for k in k_values}
        
        # Compute similarities
        similarities = cosine_similarity(query_embeddings, candidate_embeddings)
        
        for i in range(n_queries):
            # Get similarity scores for query i
            query_similarities = similarities[i]
            
            # Get top-k indices (sorted by similarity descending)
            top_k_indices = np.argsort(query_similarities)[::-1]
            
            # Check if true positive is in top-k for each k
            true_idx = true_indices[i]
            
            for k in k_values:
                if true_idx in top_k_indices[:k]:
                    hit_scores[k] += 1
        
        # Convert to percentages
        for k in k_values:
            hit_scores[k] = (hit_scores[k] / n_queries) * 100
            
        return hit_scores
    
    def prepare_retrieval_task(self, data: pd.DataFrame, sample_size: int = 1000) -> Tuple[List[str], List[str], List[int]]:
        """
        Prepare retrieval task tá»« ViSTS data
        
        Returns:
            queries: List of query sentences
            candidates: List of candidate sentences  
            true_indices: True positive indices for each query
        """
        print(f"ğŸ”„ Preparing retrieval task with {sample_size} samples...")
        
        # Sample data for faster testing
        if len(data) > sample_size:
            sampled_data = data.sample(n=sample_size, random_state=42)
        else:
            sampled_data = data.copy()
        
        queries = []
        candidates = []
        true_indices = []
        
        # Create candidate pool from all sentence2s
        all_sentence2s = sampled_data['sentence2'].tolist()
        
        for idx, row in sampled_data.iterrows():
            query = row['sentence1']
            true_candidate = row['sentence2']
            
            # Find index of true candidate in candidate pool
            try:
                true_idx = all_sentence2s.index(true_candidate)
            except ValueError:
                # If not found, skip this sample
                continue
                
            queries.append(query)
            true_indices.append(true_idx)
        
        candidates = all_sentence2s
        
        print(f"âœ… Prepared {len(queries)} queries with {len(candidates)} candidates")
        return queries, candidates, true_indices
    
    def test_model(self, model_name: str, model_info: Dict) -> Dict:
        """
        Test má»™t model vá»›i Hit@K metrics
        """
        print(f"\nğŸ§ª Testing {model_info['name']} ({model_name})")
        
        # Load model
        model = self.load_model_safe(model_name)
        if model is None:
            return {
                'model_name': model_name,
                'display_name': model_info['name'],
                'status': 'failed',
                'error': 'Failed to load model'
            }
        
        try:
            # Prepare data
            positive_data = self.dataset['positive_pairs']
            try:
                sample_size = RETRIEVAL_SAMPLE_SIZE
            except NameError:
                sample_size = 500
            queries, candidates, true_indices = self.prepare_retrieval_task(positive_data, sample_size=sample_size)
            
            if len(queries) == 0:
                return {
                    'model_name': model_name,
                    'display_name': model_info['name'], 
                    'status': 'failed',
                    'error': 'No valid query-candidate pairs'
                }
            
            # Encode queries and candidates
            print(f"   Encoding {len(queries)} queries...")
            start_time = time.time()
            query_embeddings = model.encode(queries, convert_to_numpy=True, show_progress_bar=False)
            query_time = time.time() - start_time
            
            print(f"   Encoding {len(candidates)} candidates...")
            start_time = time.time()
            candidate_embeddings = model.encode(candidates, convert_to_numpy=True, show_progress_bar=False)
            candidate_time = time.time() - start_time
            
            # Compute Hit@K
            print(f"   Computing Hit@K metrics...")
            try:
                k_values = HIT_AT_K_VALUES
            except NameError:
                k_values = [1, 4, 10]
            
            hit_scores = self.compute_hit_at_k(
                query_embeddings, 
                candidate_embeddings, 
                true_indices,
                k_values=k_values
            )
            
            total_time = query_time + candidate_time
            
            print(f"   âœ… Results: Hit@1={hit_scores[1]:.1f}%, Hit@4={hit_scores[4]:.1f}%, Hit@10={hit_scores[10]:.1f}%")
            print(f"   â±ï¸  Total time: {total_time:.2f}s")
            
            return {
                'model_name': model_name,
                'display_name': model_info['name'],
                'hit_at_1': hit_scores[1],
                'hit_at_4': hit_scores[4], 
                'hit_at_10': hit_scores[10],
                'query_time': query_time,
                'candidate_time': candidate_time,
                'total_time': total_time,
                'n_queries': len(queries),
                'n_candidates': len(candidates),
                'status': 'success'
            }
            
        except Exception as e:
            print(f"   âŒ Error testing model: {e}")
            return {
                'model_name': model_name,
                'display_name': model_info['name'],
                'status': 'failed', 
                'error': str(e)
            }
    
    def run_benchmark(self) -> Dict:
        """
        Cháº¡y benchmark cho táº¥t cáº£ models
        """
        print("ğŸš€ Starting Hit@K Benchmark...")
        
        # Load dataset
        if not self.load_vists_dataset():
            print("âŒ Cannot load dataset, aborting benchmark")
            return {}
        
        # Test each model
        for model_name, model_info in self.models_to_test.items():
            result = self.test_model(model_name, model_info)
            self.results[model_name] = result
        
        return self.results
    
    def create_results_dataframe(self) -> pd.DataFrame:
        """
        Convert results to DataFrame
        """
        rows = []
        
        for model_name, result in self.results.items():
            if result['status'] == 'success':
                rows.append({
                    'Model': result['display_name'],
                    'Model Path': model_name,
                    'Hit@1': result['hit_at_1'],
                    'Hit@4': result['hit_at_4'],
                    'Hit@10': result['hit_at_10'],
                    'Total Time (s)': result['total_time'],
                    'Queries': result['n_queries'],
                    'Candidates': result['n_candidates'],
                    'Status': 'Success'
                })
            else:
                rows.append({
                    'Model': result['display_name'],
                    'Model Path': model_name,
                    'Hit@1': 0,
                    'Hit@4': 0, 
                    'Hit@10': 0,
                    'Total Time (s)': 0,
                    'Queries': 0,
                    'Candidates': 0,
                    'Status': f"Failed: {result.get('error', 'Unknown')}"
                })
        
        return pd.DataFrame(rows)
    
    def plot_results(self, df: pd.DataFrame, save_path: str = None):
        """
        Create visualization nhÆ° hÃ¬nh benchmark
        """
        # Filter successful models
        successful_df = df[df['Status'] == 'Success'].copy()
        
        if len(successful_df) == 0:
            print("âŒ No successful models to plot")
            return
        
        # Sort by Hit@1 descending
        successful_df = successful_df.sort_values('Hit@1', ascending=True)
        
        # Create plot
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Set positions
        y_pos = np.arange(len(successful_df))
        
        # Plot Hit@1 and Hit@4
        bars1 = ax.barh(y_pos - 0.2, successful_df['Hit@1'], 0.4, 
                       label='Hit@1', color='#FF8C00', alpha=0.8)
        bars4 = ax.barh(y_pos + 0.2, successful_df['Hit@4'], 0.4,
                       label='Hit@4', color='#FF6347', alpha=0.8)
        
        # Customize plot
        ax.set_yticks(y_pos)
        ax.set_yticklabels(successful_df['Model'])
        ax.set_xlabel('Score (%)')
        ax.set_title('Hit@1 and Hit@4 for Different Embedding Models (Sorted Ascending)', 
                    fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_xlim(0, 100)
        
        # Add value labels on bars
        for bar in bars1:
            width = bar.get_width()
            ax.text(width + 1, bar.get_y() + bar.get_height()/2, 
                   f'{width:.1f}%', ha='left', va='center', fontsize=9)
        
        for bar in bars4:
            width = bar.get_width()
            ax.text(width + 1, bar.get_y() + bar.get_height()/2,
                   f'{width:.1f}%', ha='left', va='center', fontsize=9)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Chart saved to: {save_path}")
        
        plt.show()
    
    def save_results(self, df: pd.DataFrame, timestamp: str = None):
        """
        Save results to files
        """
        if timestamp is None:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Get current script directory
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Save detailed results
        results_file = os.path.join(script_dir, f"hit_at_k_benchmark_results.json")
        import json
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # Save summary CSV
        csv_file = os.path.join(script_dir, f"hit_at_k_benchmark_summary.csv")
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # Save chart
        chart_file = os.path.join(script_dir, f"hit_at_k_benchmark_chart.png")
        self.plot_results(df, save_path=chart_file)
        
        print(f"ğŸ’¾ Results saved:")
        print(f"   - {results_file} (detailed JSON)")
        print(f"   - {csv_file} (summary CSV)")
        print(f"   - {chart_file} (chart PNG)")

def main():
    """
    Main function to run Hit@K benchmark
    """
    print("ğŸ¯ Hit@K Benchmark for Vietnamese Embedding Models")
    print("=" * 60)
    
    # Initialize benchmark
    benchmark = HitAtKBenchmark()
    
    # Run benchmark
    results = benchmark.run_benchmark()
    
    if not results:
        print("âŒ No results to analyze")
        return
    
    # Create results DataFrame
    df = benchmark.create_results_dataframe()
    
    # Display results
    print("\nğŸ“Š BENCHMARK RESULTS:")
    print("=" * 60)
    print(df.to_string(index=False))
    
    # Show top performers
    successful_df = df[df['Status'] == 'Success']
    if len(successful_df) > 0:
        print(f"\nğŸ† TOP PERFORMERS:")
        print("-" * 30)
        
        # Sort by Hit@1
        top_hit1 = successful_df.nlargest(3, 'Hit@1')
        print("ğŸ“ˆ Best Hit@1:")
        for i, (_, row) in enumerate(top_hit1.iterrows(), 1):
            print(f"   {i}. {row['Model']}: {row['Hit@1']:.1f}%")
        
        # Sort by Hit@4  
        top_hit4 = successful_df.nlargest(3, 'Hit@4')
        print("\nğŸ“ˆ Best Hit@4:")
        for i, (_, row) in enumerate(top_hit4.iterrows(), 1):
            print(f"   {i}. {row['Model']}: {row['Hit@4']:.1f}%")
    
    # Save results
    benchmark.save_results(df)
    
    print("\nâœ… Benchmark completed!")

if __name__ == "__main__":
    main()
