#!/usr/bin/env python3
"""
Embedding Manager cho Mental Health RAG Agent
Qu·∫£n l√Ω embeddings v·ªõi t·ªëi ∆∞u h√≥a cho n·ªôi dung t√¢m l√Ω ti·∫øng Vi·ªát
"""

import numpy as np
from typing import List, Dict, Optional, Union
from sentence_transformers import SentenceTransformer
import torch
from config import Config, MENTAL_HEALTH_KEYWORDS

class EmbeddingManager:
    def __init__(self):
        """
        Kh·ªüi t·∫°o Embedding Manager v·ªõi model t·ªëi ∆∞u cho ti·∫øng Vi·ªát + thu·∫≠t ng·ªØ t√¢m l√Ω
        """
        print(f"üßÆ Kh·ªüi t·∫°o Embedding Manager...")
        print(f"   Model: {Config.EMBEDDING_MODEL}")
        
        # Load model
        try:
            print(f"   Loading {Config.EMBEDDING_MODEL}...")
            # Some models require trust_remote_code=True
            try:
                self.model = SentenceTransformer(Config.EMBEDDING_MODEL)
            except ValueError as ve:
                if "trust_remote_code" in str(ve):
                    print(f"   ‚ö†Ô∏è  Model requires trust_remote_code=True, retrying...")
                    self.model = SentenceTransformer(Config.EMBEDDING_MODEL, trust_remote_code=True)
                else:
                    raise ve
            self.model_name = Config.EMBEDDING_MODEL
            
            # Get embedding dimension v·ªõi text ti·∫øng Vi·ªát
            test_texts = ["Xin ch√†o", "test", "s·ª©c kh·ªèe t√¢m l√Ω"]
            sample_embeddings = self.model.encode(test_texts, convert_to_numpy=True)
            
            if len(sample_embeddings.shape) == 1:
                # Single embedding
                self.embedding_dimension = len(sample_embeddings)
            else:
                # Batch embeddings
                self.embedding_dimension = sample_embeddings.shape[1]
            
            print(f"‚úÖ ƒê√£ load embedding model th√†nh c√¥ng")
            print(f"   Dimension: {self.embedding_dimension}")
            print(f"   Device: {self.model.device}")
            print(f"   Test embeddings shape: {sample_embeddings.shape}")
            
        except Exception as e:
            print(f"‚ùå L·ªói load embedding model: {e}")
            print(f"   Model: {Config.EMBEDDING_MODEL}")
            print(f"   Error type: {type(e).__name__}")
            raise
    
    def preprocess_text_for_embedding(self, text: str) -> str:
        """
        Ti·ªÅn x·ª≠ l√Ω text c∆° b·∫£n cho embedding - kh√¥ng th√™m domain-specific keywords
        """
        if not text:
            return ""
        
        # Clean v√† truncate text ƒë·ªÉ tr√°nh l·ªói v·ªõi Vietnamese model
        cleaned_text = text.strip()
        
        # Remove ho·∫∑c replace c√°c k√Ω t·ª± c√≥ th·ªÉ g√¢y v·∫•n ƒë·ªÅ
        import re
        # Remove control characters v√† non-printable chars
        cleaned_text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', ' ', cleaned_text)
        # Normalize whitespace
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        # Truncate text n·∫øu qu√° d√†i (Vietnamese model c√≥ th·ªÉ c√≥ gi·ªõi h·∫°n)
        max_chars = 512  # Conservative limit cho Vietnamese model
        if len(cleaned_text) > max_chars:
            # C·∫Øt ·ªü boundary c·ªßa c√¢u ƒë·ªÉ gi·ªØ ng·ªØ nghƒ©a
            sentences = cleaned_text.split('. ')
            truncated = ""
            for sentence in sentences:
                if len(truncated + sentence + '. ') <= max_chars:
                    truncated += sentence + '. '
                else:
                    break
            cleaned_text = truncated.strip()
            if not cleaned_text.endswith('.'):
                cleaned_text += '.'
        
        # Final validation
        if not cleaned_text or len(cleaned_text.strip()) < 3:
            return ""
            
        return cleaned_text
    
    def embed_query(self, query: str) -> np.ndarray:
        """
        T·∫°o embedding cho query v·ªõi ti·ªÅn x·ª≠ l√Ω ƒë·∫∑c bi·ªát
        """
        if not query.strip():
            return np.zeros(self.embedding_dimension)
        
        try:
            # Ch·ªâ ti·ªÅn x·ª≠ l√Ω query m√† kh√¥ng th√™m metadata type
            processed_query = self.preprocess_text_for_embedding(query)
            
            # T·∫°o embedding thu·∫ßn t√∫y t·ª´ n·ªôi dung
            embedding = self.model.encode(
                processed_query,
                convert_to_numpy=True,
                normalize_embeddings=Config.NORMALIZE_EMBEDDINGS
            )
            
            return embedding
            
        except Exception as e:
            print(f"‚ùå L·ªói t·∫°o embedding cho query: {e}")
            return np.zeros(self.embedding_dimension)
    
    def embed_document(self, document: Dict) -> np.ndarray:
        """
        T·∫°o embedding cho document thu·∫ßn t√∫y t·ª´ n·ªôi dung
        """
        try:
            content = document.get("content", "")
            
            if not content.strip():
                return np.zeros(self.embedding_dimension)
            
            # Ch·ªâ ti·ªÅn x·ª≠ l√Ω content m√† kh√¥ng th√™m metadata type
            processed_content = self.preprocess_text_for_embedding(content)
            
            # T·∫°o embedding thu·∫ßn t√∫y t·ª´ n·ªôi dung
            embedding = self.model.encode(
                processed_content,
                convert_to_numpy=True,
                normalize_embeddings=Config.NORMALIZE_EMBEDDINGS
            )
            
            return embedding
            
        except Exception as e:
            print(f"‚ùå L·ªói t·∫°o embedding cho document: {e}")
            return np.zeros(self.embedding_dimension)
    
    def embed_documents(self, documents: List[Dict]) -> List[Dict]:
        """
        T·∫°o embeddings cho danh s√°ch documents v·ªõi batch processing
        """
        if not documents:
            return []
        
        print(f"üßÆ T·∫°o embeddings cho {len(documents)} documents...")
        
        documents_with_embeddings = []
        batch_size = Config.EMBEDDING_BATCH_SIZE
        
        try:
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i + batch_size]
                print(f"   Batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}: {len(batch)} documents")
                
                # T·∫°o embeddings cho batch
                batch_texts = []
                for doc in batch:
                    content = doc.get("content", "")
                    
                    # Ch·ªâ ti·ªÅn x·ª≠ l√Ω content m√† kh√¥ng th√™m metadata type
                    processed_content = self.preprocess_text_for_embedding(content)
                    batch_texts.append(processed_content)
                
                # Batch encoding v·ªõi error handling t·ªët h∆°n
                try:
                    # T·∫°o mapping gi·ªØa valid texts v√† original indices
                    valid_items = []
                    for j, text in enumerate(batch_texts):
                        if text.strip():
                            valid_items.append((j, text.strip()))
                    
                    if not valid_items:
                        print(f"   ‚ö†Ô∏è  Batch {i//batch_size + 1} kh√¥ng c√≥ text h·ª£p l·ªá")
                        continue
                    
                    # Extract ch·ªâ valid texts
                    valid_texts = [item[1] for item in valid_items]
                    
                    # Debug info
                    print(f"   üìä Batch {i//batch_size + 1}: {len(batch_texts)} total, {len(valid_texts)} valid")
                    
                    # Safe encoding v·ªõi Vietnamese model
                    try:
                        batch_embeddings = self.model.encode(
                            valid_texts,
                            convert_to_numpy=True,
                            normalize_embeddings=Config.NORMALIZE_EMBEDDINGS,
                            batch_size=min(len(valid_texts), 8),  # Smaller batch for Vietnamese model
                            show_progress_bar=False
                        )
                    except Exception as encode_error:
                        print(f"   ‚ö†Ô∏è  Batch encoding failed: {encode_error}")
                        # Try with even smaller batch
                        if len(valid_texts) > 1:
                            print(f"   üîÑ Retrying with batch_size=1...")
                            batch_embeddings = self.model.encode(
                                valid_texts,
                                convert_to_numpy=True,
                                normalize_embeddings=Config.NORMALIZE_EMBEDDINGS,
                                batch_size=1,
                                show_progress_bar=False
                            )
                        else:
                            raise encode_error
                    
                    # ƒê·∫£m b·∫£o shape ƒë√∫ng
                    if len(batch_embeddings.shape) == 1:
                        batch_embeddings = batch_embeddings.reshape(1, -1)
                    
                    print(f"   üìê Embeddings shape: {batch_embeddings.shape}, expected: ({len(valid_texts)}, {self.embedding_dimension})")
                    
                    # Ki·ªÉm tra consistency
                    if batch_embeddings.shape[0] != len(valid_texts):
                        print(f"   ‚ùå Shape mismatch: {batch_embeddings.shape[0]} embeddings vs {len(valid_texts)} texts")
                        continue
                    
                    # Th√™m embeddings v√†o documents v·ªõi mapping ch√≠nh x√°c
                    for embed_idx, (original_idx, _) in enumerate(valid_items):
                        if embed_idx >= len(batch_embeddings):
                            print(f"   ‚ùå Embedding index {embed_idx} out of range for {len(batch_embeddings)} embeddings")
                            break
                            
                        doc = batch[original_idx]
                        doc_with_embedding = doc.copy()
                        doc_with_embedding["embedding"] = batch_embeddings[embed_idx].tolist()
                        doc_with_embedding["embedding_model"] = self.model_name
                        doc_with_embedding["embedding_dimension"] = self.embedding_dimension
                        documents_with_embeddings.append(doc_with_embedding)
                    
                    print(f"   ‚úÖ Successfully processed {len(valid_items)} documents")
                            
                except Exception as batch_error:
                    print(f"   ‚ùå L·ªói batch {i//batch_size + 1}: {batch_error}")
                    print(f"   üìä Batch info: {len(batch_texts)} texts, {[len(t) for t in batch_texts[:3]]} chars")
                    
                    # Fallback: X·ª≠ l√Ω t·ª´ng document ri√™ng l·∫ª
                    print(f"   üîÑ Fallback: X·ª≠ l√Ω t·ª´ng document ri√™ng l·∫ª...")
                    for j, doc in enumerate(batch):
                        try:
                            content = doc.get("content", "").strip()
                            if not content:
                                continue
                                
                            # Validate v√† preprocess content
                            processed_content = self.preprocess_text_for_embedding(content)
                            if len(processed_content) < 10:  # Skip very short texts
                                print(f"      ‚ö†Ô∏è  Skipping too short text: {len(processed_content)} chars")
                                continue
                            
                            # Encode single document v·ªõi additional safety
                            single_embedding = self.model.encode(
                                processed_content,
                                convert_to_numpy=True,
                                normalize_embeddings=Config.NORMALIZE_EMBEDDINGS,
                                show_progress_bar=False
                            )
                            
                            # ƒê·∫£m b·∫£o l√† 1D array
                            if len(single_embedding.shape) > 1:
                                single_embedding = single_embedding.flatten()
                            
                            doc_with_embedding = doc.copy()
                            doc_with_embedding["embedding"] = single_embedding.tolist()
                            doc_with_embedding["embedding_model"] = self.model_name
                            doc_with_embedding["embedding_dimension"] = self.embedding_dimension
                            documents_with_embeddings.append(doc_with_embedding)
                            
                        except Exception as single_error:
                            print(f"      ‚ùå L·ªói document {j}: {single_error}")
                            continue
                    
                    print(f"   üîÑ Fallback completed for batch {i//batch_size + 1}")
                    continue
            
            print(f"‚úÖ ƒê√£ t·∫°o embeddings cho t·∫•t c·∫£ documents")
            return documents_with_embeddings
            
        except Exception as e:
            print(f"‚ùå L·ªói t·∫°o embeddings: {e}")
            return []
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa 2 embeddings
        """
        try:
            # Ensure embeddings are numpy arrays
            if isinstance(embedding1, list):
                embedding1 = np.array(embedding1)
            if isinstance(embedding2, list):
                embedding2 = np.array(embedding2)
            
            # Normalize if needed
            if Config.NORMALIZE_EMBEDDINGS:
                embedding1 = embedding1 / np.linalg.norm(embedding1)
                embedding2 = embedding2 / np.linalg.norm(embedding2)
            
            # Compute cosine similarity
            similarity = np.dot(embedding1, embedding2)
            return float(similarity)
            
        except Exception as e:
            print(f"‚ùå L·ªói t√≠nh similarity: {e}")
            return 0.0
    
    def find_most_similar(self, query_embedding: np.ndarray, 
                         document_embeddings: List[np.ndarray], 
                         top_k: int = None) -> List[int]:
        """
        T√¨m c√°c documents c√≥ embedding t∆∞∆°ng ƒë·ªìng nh·∫•t v·ªõi query
        """
        if top_k is None:
            top_k = Config.TOP_K_DOCUMENTS
        
        try:
            similarities = []
            for i, doc_embedding in enumerate(document_embeddings):
                similarity = self.compute_similarity(query_embedding, doc_embedding)
                similarities.append((i, similarity))
            
            # S·∫Øp x·∫øp theo ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·∫£m d·∫ßn
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # L·∫•y top k indices
            top_indices = [idx for idx, _ in similarities[:top_k]]
            return top_indices
            
        except Exception as e:
            print(f"‚ùå L·ªói t√¨m ki·∫øm similar embeddings: {e}")
            return []
    
    def get_embedding_stats(self, documents_with_embeddings: List[Dict]) -> Dict:
        """
        L·∫•y th·ªëng k√™ v·ªÅ embeddings
        """
        if not documents_with_embeddings:
            return {}
        
        embeddings = [doc["embedding"] for doc in documents_with_embeddings if "embedding" in doc]
        
        if not embeddings:
            return {}
        
        embeddings_array = np.array(embeddings)
        
        stats = {
            "total_embeddings": len(embeddings),
            "embedding_dimension": self.embedding_dimension,
            "model_name": self.model_name,
            "mean_magnitude": float(np.mean(np.linalg.norm(embeddings_array, axis=1))),
            "std_magnitude": float(np.std(np.linalg.norm(embeddings_array, axis=1))),
            "normalized": Config.NORMALIZE_EMBEDDINGS
        }
        
        return stats
    
    def test_embedding_quality(self) -> Dict:
        """
        Test ch·∫•t l∆∞·ª£ng embedding v·ªõi c√°c c√¢u m·∫´u v·ªÅ t√¢m l√Ω
        """
        print("üß™ Testing embedding quality...")
        
        test_queries = [
            "t√¥i c·∫£m th·∫•y bu·ªìn v√† m·ªát m·ªèi",
            "l√†m sao ƒë·ªÉ gi·∫£m stress h·ªçc t·∫≠p",
            "tri·ªáu ch·ª©ng c·ªßa tr·∫ßm c·∫£m l√† g√¨",
            "t√¥i g·∫∑p kh√≥ khƒÉn trong vi·ªác ng·ªß"
        ]
        
        test_documents = [
            {"content": "Tr·∫ßm c·∫£m l√† m·ªôt r·ªëi lo·∫°n t√¢m l√Ω ph·ªï bi·∫øn", "content_type": "symptom_description"},
            {"content": "C√°c k·ªπ thu·∫≠t th∆∞ gi√£n gi√∫p gi·∫£m cƒÉng th·∫≥ng", "content_type": "intervention_guidance"},
            {"content": "M·∫•t ng·ªß c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa lo √¢u", "content_type": "symptom_description"},
            {"content": "H·ªçc sinh c·∫ßn c√≥ th·ªùi gian ngh·ªâ ng∆°i h·ª£p l√Ω", "content_type": "student_focused"}
        ]
        
        results = {}
        
        try:
            # Test query embeddings
            query_embeddings = [self.embed_query(q) for q in test_queries]
            
            # Test document embeddings
            doc_embeddings = [self.embed_document(d) for d in test_documents]
            
            # Test similarities
            similarities = []
            for i, q_emb in enumerate(query_embeddings):
                for j, d_emb in enumerate(doc_embeddings):
                    sim = self.compute_similarity(q_emb, d_emb)
                    similarities.append({
                        "query": test_queries[i],
                        "document": test_documents[j]["content"][:50] + "...",
                        "similarity": sim
                    })
            
            # Find best matches
            similarities.sort(key=lambda x: x["similarity"], reverse=True)
            
            results = {
                "status": "success",
                "query_embeddings_generated": len(query_embeddings),
                "document_embeddings_generated": len(doc_embeddings),
                "top_similarities": similarities[:3],
                "embedding_dimension": self.embedding_dimension,
                "model": self.model_name
            }
            
            print("‚úÖ Embedding quality test completed")
            
        except Exception as e:
            results = {
                "status": "error",
                "error": str(e)
            }
            print(f"‚ùå Embedding quality test failed: {e}")
        
        return results
