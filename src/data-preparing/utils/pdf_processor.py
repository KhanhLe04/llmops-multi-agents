#!/usr/bin/env python3
"""
PDF Processor cho Mental Health RAG Agent
Xá»­ lÃ½ cÃ¡c tÃ i liá»‡u PDF vá» tÆ° váº¥n tÃ¢m lÃ½ há»c sinh sinh viÃªn
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Optional
import PyPDF2
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from config import Config, MENTAL_HEALTH_KEYWORDS

class PDFProcessor:
    def __init__(self):
        """
        Khá»Ÿi táº¡o PDF processor vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u cho ná»™i dung tÃ¢m lÃ½
        """
        print(f"ðŸ”§ Khá»Ÿi táº¡o PDF Processor cho domain: {Config.DOMAIN}")
        
        # Text splitter vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u cho ná»™i dung tÃ¢m lÃ½
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            # Separators tá»‘i Æ°u cho vÄƒn báº£n tiáº¿ng Viá»‡t vá» tÃ¢m lÃ½
            separators=[
                "\n\n\n",  # Section breaks
                "\n\n",    # Paragraph breaks
                "\n",      # Line breaks
                ". ",      # Sentence ends
                "ã€‚",      # Vietnamese sentence end
                "! ",      # Exclamation
                "? ",      # Question
                "; ",      # Semicolon
                ", ",      # Comma
                " "        # Space
            ]
        )
        
        print(f"âœ… PDF Processor sáºµn sÃ ng vá»›i chunk size: {Config.CHUNK_SIZE}")
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        TrÃ­ch xuáº¥t text tá»« PDF vá»›i xá»­ lÃ½ encoding cho tiáº¿ng Viá»‡t
        """
        try:
            text = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                print(f"ðŸ“– Äang Ä‘á»c PDF: {pdf_path}")
                print(f"   Sá»‘ trang: {len(pdf_reader.pages)}")
                
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        
                        # Xá»­ lÃ½ encoding vÃ  formatting cho tiáº¿ng Viá»‡t
                        page_text = self.clean_vietnamese_text(page_text)
                        
                        text += f"\n--- Trang {page_num + 1} ---\n{page_text}\n"
                        
                    except Exception as e:
                        print(f"âš ï¸  Lá»—i Ä‘á»c trang {page_num + 1}: {e}")
                        continue
                
                print(f"âœ… ÄÃ£ trÃ­ch xuáº¥t {len(text)} kÃ½ tá»± tá»« {len(pdf_reader.pages)} trang")
                return text
                
        except Exception as e:
            print(f"âŒ Lá»—i Ä‘á»c PDF {pdf_path}: {e}")
            return ""
    
    def clean_vietnamese_text(self, text: str) -> str:
        """
        LÃ m sáº¡ch vÃ  chuáº©n hÃ³a text tiáº¿ng Viá»‡t cho domain tÃ¢m lÃ½
        """
        if not text:
            return ""
        
        # XÃ³a cÃ¡c kÃ½ tá»± khÃ´ng mong muá»‘n
        text = re.sub(r'\x00', '', text)  # Null characters
        text = re.sub(r'[\x01-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)  # Control characters
        
        # Chuáº©n hÃ³a khoáº£ng tráº¯ng
        text = re.sub(r'\s+', ' ', text)  # Multiple spaces to single
        text = re.sub(r'\n\s*\n', '\n\n', text)  # Multiple newlines to double
        
        # Chuáº©n hÃ³a dáº¥u cÃ¢u tiáº¿ng Viá»‡t
        text = re.sub(r'\s*\.\s*', '. ', text)
        text = re.sub(r'\s*,\s*', ', ', text)
        text = re.sub(r'\s*;\s*', '; ', text)
        text = re.sub(r'\s*:\s*', ': ', text)
        text = re.sub(r'\s*\?\s*', '? ', text)
        text = re.sub(r'\s*!\s*', '! ', text)
        
        # Xá»­ lÃ½ sá»‘ trang vÃ  header/footer
        text = re.sub(r'Trang \d+', '', text)
        text = re.sub(r'Page \d+', '', text)
        
        # Loáº¡i bá» cÃ¡c kÃ½ tá»± láº·p láº¡i khÃ´ng cáº§n thiáº¿t
        text = re.sub(r'[_-]{3,}', '', text)
        text = re.sub(r'[.]{3,}', '...', text)
        
        return text.strip()
    
    def enhance_mental_health_content(self, text: str) -> str:
        """
        KhÃ´ng thÃªm keyword enhancement - Ä‘á»ƒ semantic model tá»± há»c
        """
        # Return text gá»‘c, khÃ´ng thÃªm artificial markers
        return text
    
    def create_chunks(self, text: str, source_file: str) -> List[Dict]:
        """
        Chia text thÃ nh chunks vá»›i metadata cho tÃ¢m lÃ½ há»c
        """
        if not text.strip():
            return []
        
        print(f"ðŸ“ Äang chia text thÃ nh chunks...")
        print(f"   Text length: {len(text)} kÃ½ tá»±")
        
        # TÄƒng cÆ°á»ng ná»™i dung trÆ°á»›c khi chia chunks
        enhanced_text = self.enhance_mental_health_content(text)
        
        # Táº¡o Document object
        doc = Document(
            page_content=enhanced_text,
            metadata={
                "source": source_file,
                "domain": Config.DOMAIN,
                "type": "mental_health_document"
            }
        )
        
        # Chia thÃ nh chunks
        chunks = self.text_splitter.split_documents([doc])
        
        # Chuyá»ƒn Ä‘á»•i thÃ nh format dictionary vá»›i metadata Ä‘áº§y Ä‘á»§
        chunk_dicts = []
        for i, chunk in enumerate(chunks):
            # PhÃ¢n loáº¡i ná»™i dung chunk
            content_type = self.classify_chunk_content(chunk.page_content)
            
            chunk_dict = {
                "content": chunk.page_content,
                "source": source_file,
                "chunk_id": i,
                "content_type": content_type,
                "metadata": {
                    **chunk.metadata,
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "char_count": len(chunk.page_content),
                    "word_count": len(chunk.page_content.split()),
                    "content_classification": content_type
                }
            }
            chunk_dicts.append(chunk_dict)
        
        print(f"âœ… ÄÃ£ táº¡o {len(chunk_dicts)} chunks")
        return chunk_dicts
    
    def classify_chunk_content(self, content: str) -> str:
        """
        PhÃ¢n loáº¡i ná»™i dung chunk Ä‘Æ¡n giáº£n - trÃ¡nh bias tá»« keyword matching
        """
        # Táº¥t cáº£ content Ä‘á»u Ä‘Æ°á»£c classify lÃ  general Ä‘á»ƒ semantic search tá»± quyáº¿t Ä‘á»‹nh
        return "general_content"
    
    def process_pdf(self, pdf_path: str) -> List[Dict]:
        """
        Xá»­ lÃ½ hoÃ n chá»‰nh má»™t file PDF
        """
        if not os.path.exists(pdf_path):
            print(f"âŒ File khÃ´ng tá»“n táº¡i: {pdf_path}")
            return []
        
        print(f"ðŸ”„ Báº¯t Ä‘áº§u xá»­ lÃ½ PDF: {Path(pdf_path).name}")
        
        try:
            # BÆ°á»›c 1: TrÃ­ch xuáº¥t text
            text = self.extract_text_from_pdf(pdf_path)
            
            if not text.strip():
                print(f"âš ï¸  KhÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c text tá»«: {pdf_path}")
                return []
            
            # BÆ°á»›c 2: Táº¡o chunks
            chunks = self.create_chunks(text, Path(pdf_path).name)
            
            if chunks:
                print(f"âœ… ÄÃ£ xá»­ lÃ½ xong PDF: {len(chunks)} chunks")
                
                # In thá»‘ng kÃª
                content_types = {}
                for chunk in chunks:
                    ctype = chunk["content_type"]
                    content_types[ctype] = content_types.get(ctype, 0) + 1
                
                print(f"ðŸ“Š Thá»‘ng kÃª ná»™i dung:")
                for ctype, count in content_types.items():
                    print(f"   - {ctype}: {count} chunks")
            
            return chunks
            
        except Exception as e:
            print(f"âŒ Lá»—i xá»­ lÃ½ PDF {pdf_path}: {e}")
            return []
    
    def get_processing_stats(self, chunks: List[Dict]) -> Dict:
        """
        Láº¥y thá»‘ng kÃª xá»­ lÃ½
        """
        if not chunks:
            return {"total_chunks": 0}
        
        stats = {
            "total_chunks": len(chunks),
            "total_characters": sum(len(chunk["content"]) for chunk in chunks),
            "total_words": sum(chunk["metadata"]["word_count"] for chunk in chunks),
            "content_types": {},
            "sources": list(set(chunk["source"] for chunk in chunks))
        }
        
        # Thá»‘ng kÃª theo loáº¡i ná»™i dung
        for chunk in chunks:
            ctype = chunk["content_type"]
            stats["content_types"][ctype] = stats["content_types"].get(ctype, 0) + 1
        
        return stats
