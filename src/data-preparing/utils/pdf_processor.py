#!/usr/bin/env python3
"""
PDF Processor cho Mental Health RAG Agent
X·ª≠ l√Ω c√°c t√†i li·ªáu PDF v·ªÅ t∆∞ v·∫•n t√¢m l√Ω h·ªçc sinh sinh vi√™n
"""

import os
import re
import uuid
from pathlib import Path
from typing import List, Dict, Optional
import PyPDF2
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from config import Config

class PDFProcessor:
    def __init__(self):
        """
        Kh·ªüi t·∫°o PDF processor
        """
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            separators=[
                "\n\n\n",  # Section breaks
                "\n\n",    # Paragraph breaks
                "\n",      # Line breaks
                ". ",      # Sentence ends
                "! ",      # Exclamation
                "? ",      # Question
                "; ",      # Semicolon
                ", ",      # Comma
                " "        # Space
            ]
        )
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        Tr√≠ch xu·∫•t text t·ª´ PDF v·ªõi x·ª≠ l√Ω encoding cho ti·∫øng Vi·ªát
        """
        try:
            text = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                print(f"üìñ ƒêang ƒë·ªçc PDF: {pdf_path}")
                print(f"   S·ªë trang: {len(pdf_reader.pages)}")
                
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        
                        # X·ª≠ l√Ω encoding v√† formatting cho ti·∫øng Vi·ªát
                        page_text = self.clean_vietnamese_text(page_text)
                        
                        # Th√™m page marker ƒë·ªÉ x·ª≠ l√Ω sau
                        text += f"\n---PAGE_{page_num + 1}---\n{page_text}\n"
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è  L·ªói ƒë·ªçc trang {page_num + 1}: {e}")
                        continue
                
                print(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t {len(text)} k√Ω t·ª± t·ª´ {len(pdf_reader.pages)} trang")
                return text
                
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc PDF {pdf_path}: {e}")
            return ""
    
    def clean_vietnamese_text(self, text: str) -> str:
        """
        L√†m s·∫°ch v√† chu·∫©n h√≥a text ti·∫øng Vi·ªát cho domain t√¢m l√Ω
        """
        if not text:
            return ""
        
        # X√≥a c√°c k√Ω t·ª± kh√¥ng mong mu·ªën
        text = re.sub(r'\x00', '', text)  # Null characters
        text = re.sub(r'[\x01-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)  # Control characters
        
        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        text = re.sub(r'\s+', ' ', text)  # Multiple spaces to single
        text = re.sub(r'\n\s*\n', '\n\n', text)  # Multiple newlines to double
        
        # Chu·∫©n h√≥a d·∫•u c√¢u ti·∫øng Vi·ªát
        text = re.sub(r'\s*\.\s*', '. ', text)
        text = re.sub(r'\s*,\s*', ', ', text)
        text = re.sub(r'\s*;\s*', '; ', text)
        text = re.sub(r'\s*:\s*', ': ', text)
        text = re.sub(r'\s*\?\s*', '? ', text)
        text = re.sub(r'\s*!\s*', '! ', text)
        
        # X·ª≠ l√Ω s·ªë trang v√† header/footer (gi·ªØ l·∫°i page markers ƒë·ªÉ x·ª≠ l√Ω ri√™ng)
        text = re.sub(r'(?<!---PAGE_)\bTrang \d+\b', '', text)
        text = re.sub(r'(?<!---PAGE_)\bPage \d+\b', '', text)
        
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± l·∫∑p l·∫°i kh√¥ng c·∫ßn thi·∫øt
        text = re.sub(r'[_-]{3,}', '', text)
        text = re.sub(r'[.]{3,}', '...', text)
        
        return text.strip()
    
    def extract_section_from_content(self, content: str) -> str:
        """
        Tr√≠ch xu·∫•t section/m·ª•c l·ª•c t·ª´ n·ªôi dung chunk
        """
        # T√¨m c√°c pattern section headers
        section_patterns = [
            r'^([IVX]+\.\s*.+?)(?:\n|$)',  # Roman numerals: I. II. III.
            r'^(\d+\.\s*.+?)(?:\n|$)',     # Numbers: 1. 2. 3.
            r'^([A-Z]\.\s*.+?)(?:\n|$)',   # Letters: A. B. C.
            r'^(CH∆Ø∆†NG\s+\d+.+?)(?:\n|$)', # Vietnamese chapters
            r'^(Ph·∫ßn\s+\d+.+?)(?:\n|$)',   # Vietnamese parts
            r'^(M·ª•c\s+\d+.+?)(?:\n|$)',    # Vietnamese sections
        ]
        
        for pattern in section_patterns:
            match = re.search(pattern, content, re.MULTILINE | re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Fallback: l·∫•y d√≤ng ƒë·∫ßu ti√™n n·∫øu ng·∫Øn (c√≥ th·ªÉ l√† title)
        first_line = content.split('\n')[0].strip()
        if len(first_line) < 100 and first_line:
            return first_line
        
        return "N·ªôi dung chung"
    
    def separate_page_numbers(self, content: str) -> tuple[str, list]:
        """
        T√°ch page numbers ra kh·ªèi content
        """
        page_numbers = []
        
        # T√¨m v√† extract page markers
        page_pattern = r'---PAGE_(\d+)---'
        matches = re.findall(page_pattern, content)
        page_numbers.extend([int(p) for p in matches])
        
        # Lo·∫°i b·ªè page markers kh·ªèi content
        clean_content = re.sub(page_pattern, '', content)
        
        # Lo·∫°i b·ªè c√°c s·ªë trang c√≤n s√≥t l·∫°i
        clean_content = re.sub(r'\n\s*---\s*Trang\s+\d+\s*---\s*\n', '\n', clean_content)
        clean_content = re.sub(r'\bTrang\s+\d+\b', '', clean_content)
        
        # Cleanup whitespace
        clean_content = re.sub(r'\n\s*\n\s*\n', '\n\n', clean_content)
        clean_content = clean_content.strip()
        
        return clean_content, page_numbers
    
    def create_chunks(self, text: str, source_file: str) -> List[Dict]:
        """
        Chia text th√†nh chunks v·ªõi metadata ƒë∆°n gi·∫£n
        """
        if not text.strip():
            return []
        
        # Generate document ID
        doc_id = str(uuid.uuid4())
        
        # T·∫°o Document object
        doc = Document(
            page_content=text,
            metadata={"source": source_file}
        )
        
        # Chia th√†nh chunks
        chunks = self.text_splitter.split_documents([doc])
        
        # Chuy·ªÉn ƒë·ªïi th√†nh format dictionary v·ªõi metadata t·ªëi gi·∫£n
        chunk_dicts = []
        for i, chunk in enumerate(chunks):
            # T√°ch page numbers ra kh·ªèi content
            clean_content, page_numbers = self.separate_page_numbers(chunk.page_content)
            
            # Extract section t·ª´ content
            section = self.extract_section_from_content(clean_content)
            
            chunk_dict = {
                "content": clean_content,
                "source": source_file,
                "chunk_index": i,
                "doc_id": doc_id,
                "section": section
            }
            chunk_dicts.append(chunk_dict)
        
        return chunk_dicts
    
    
    def process_pdf(self, pdf_path: str) -> List[Dict]:
        """
        X·ª≠ l√Ω ho√†n ch·ªânh m·ªôt file PDF
        """
        if not os.path.exists(pdf_path):
            print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {pdf_path}")
            return []
        
        print(f"üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω PDF: {Path(pdf_path).name}")
        
        try:
            # B∆∞·ªõc 1: Tr√≠ch xu·∫•t text
            text = self.extract_text_from_pdf(pdf_path)
            
            if not text.strip():
                print(f"‚ö†Ô∏è  Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c text t·ª´: {pdf_path}")
                return []
            
            # B∆∞·ªõc 2: T·∫°o chunks
            chunks = self.create_chunks(text, Path(pdf_path).name)
            
            if chunks:
                print(f"‚úÖ ƒê√£ x·ª≠ l√Ω xong PDF: {len(chunks)} chunks")
                
                # In th·ªëng k√™ sections
                sections = {}
                for chunk in chunks:
                    section = chunk["section"]
                    sections[section] = sections.get(section, 0) + 1
                
                print(f"üìä Th·ªëng k√™ sections:")
                for section, count in sections.items():
                    print(f"   - {section}: {count} chunks")
            
            return chunks
            
        except Exception as e:
            print(f"‚ùå L·ªói x·ª≠ l√Ω PDF {pdf_path}: {e}")
            return []
    
    def get_processing_stats(self, chunks: List[Dict]) -> Dict:
        """
        L·∫•y th·ªëng k√™ x·ª≠ l√Ω ƒë∆°n gi·∫£n
        """
        if not chunks:
            return {"total_chunks": 0}
        
        stats = {
            "total_chunks": len(chunks),
            "total_characters": sum(len(chunk["content"]) for chunk in chunks),
            "sections": {},
            "sources": list(set(chunk["source"] for chunk in chunks)),
            "doc_ids": list(set(chunk["doc_id"] for chunk in chunks))
        }
        
        # Th·ªëng k√™ theo sections
        for chunk in chunks:
            section = chunk["section"]
            stats["sections"][section] = stats["sections"].get(section, 0) + 1
        
        return stats
