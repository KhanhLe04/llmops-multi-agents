#!/usr/bin/env python3
"""
Qdrant Manager cho Mental Health RAG Agent
Qu·∫£n l√Ω vector database v·ªõi t·ªëi ∆∞u h√≥a cho domain t√¢m l√Ω
"""

import os
from typing import List, Dict, Optional, Any
from qdrant_client import QdrantClient
from qdrant_client.models import (
    VectorParams, Distance, CollectionStatus,
    PointStruct, Filter, FieldCondition, Range,
    MatchValue, SearchRequest, ScoredPoint
)
import uuid
from config import Config, MENTAL_HEALTH_KEYWORDS

class QdrantManager:
    def __init__(self):
        """
        Kh·ªüi t·∫°o Qdrant Manager cho mental health domain
        """
        print(f"üóÑÔ∏è  Kh·ªüi t·∫°o Qdrant Manager...")
        print(f"   URL: {Config.QDRANT_URL}")
        print(f"   Collection: {Config.COLLECTION_NAME}")
        
        try:
            # Kh·ªüi t·∫°o client
            self.client = QdrantClient(
                url=Config.QDRANT_URL,
                api_key=Config.QDRANT_API_KEY if Config.QDRANT_API_KEY else None
            )
            
            self.collection_name = Config.COLLECTION_NAME
            
            # Test connection
            self._test_connection()
            
            print(f"‚úÖ K·∫øt n·ªëi Qdrant th√†nh c√¥ng")
            
        except Exception as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi Qdrant: {e}")
            raise
    
    def _test_connection(self):
        """Test k·∫øt n·ªëi v·ªõi Qdrant"""
        try:
            collections = self.client.get_collections()
            print(f"üìä T√¨m th·∫•y {len(collections.collections)} collections")
        except Exception as e:
            raise Exception(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi Qdrant: {e}")
    
    def create_collection(self, vector_size: int, force_recreate: bool = False):
        """
        T·∫°o collection v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u cho mental health documents
        """
        try:
            print(f"üìÅ T·∫°o collection: {self.collection_name}")
            print(f"   Vector size: {vector_size}")
            
            # Ki·ªÉm tra collection c√≥ t·ªìn t·∫°i kh√¥ng
            existing_collections = self.client.get_collections().collections
            collection_exists = any(col.name == self.collection_name for col in existing_collections)
            
            if collection_exists:
                if force_recreate:
                    print(f"üóëÔ∏è  X√≥a collection c≈©...")
                    self.client.delete_collection(self.collection_name)
                else:
                    print(f"‚úÖ Collection ƒë√£ t·ªìn t·∫°i")
                    return True
            
            # T·∫°o collection m·ªõi v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=Distance.COSINE,  # Cosine distance t·ªët cho text embeddings
                    on_disk=True  # L∆∞u tr√™n disk ƒë·ªÉ ti·∫øt ki·ªám RAM
                ),
                # Optimizers configuration
                optimizers_config={
                    "deleted_threshold": 0.2,
                    "vacuum_min_vector_number": 1000,
                    "default_segment_number": 0,
                    "max_segment_size": 20000,
                    "memmap_threshold": 50000,
                    "indexing_threshold": 20000,
                    "flush_interval_sec": 5,
                    "max_optimization_threads": 1
                },
                # HNSW configuration t·ªëi ∆∞u cho retrieval accuracy
                hnsw_config={
                    "m": 16,  # S·ªë connections, c√¢n b·∫±ng accuracy v√† memory
                    "ef_construct": 100,  # Build time parameter
                    "full_scan_threshold": 10000,
                    "max_indexing_threads": 0,
                    "on_disk": False,
                    "payload_m": 16
                }
            )
            
            print(f"‚úÖ ƒê√£ t·∫°o collection th√†nh c√¥ng")
            
            # Wait for collection to be ready
            import time
            max_wait = 30
            wait_time = 0
            while wait_time < max_wait:
                try:
                    info = self.client.get_collection(self.collection_name)
                    if info.status == CollectionStatus.GREEN:
                        print(f"‚úÖ Collection s·∫µn s√†ng")
                        break
                except:
                    pass
                time.sleep(1)
                wait_time += 1
            
            return True
            
        except Exception as e:
            print(f"‚ùå L·ªói t·∫°o collection: {e}")
            return False
    
    def add_documents(self, documents_with_embeddings: List[Dict]):
        """
        Th√™m documents v√†o collection v·ªõi metadata phong ph√∫
        """
        if not documents_with_embeddings:
            print("‚ö†Ô∏è  Kh√¥ng c√≥ documents ƒë·ªÉ th√™m")
            return False
        
        print(f"üíæ Th√™m {len(documents_with_embeddings)} documents v√†o collection...")
        
        try:
            points = []
            for i, doc in enumerate(documents_with_embeddings):
                if "embedding" not in doc:
                    print(f"‚ö†Ô∏è  Document {i} kh√¥ng c√≥ embedding, b·ªè qua")
                    continue
                
                # T·∫°o payload v·ªõi metadata phong ph√∫
                payload = {
                    "content": doc["content"],
                    "source": doc["source"],
                    "chunk_id": doc["chunk_id"],
                    "content_type": doc["content_type"],
                    
                    # Metadata t·ª´ processing
                    "char_count": doc["metadata"]["char_count"],
                    "word_count": doc["metadata"]["word_count"],
                    "chunk_index": doc["metadata"]["chunk_index"],
                    "total_chunks": doc["metadata"]["total_chunks"],
                    
                    # Domain-specific metadata
                    "domain": Config.DOMAIN,
                    "embedding_model": doc["embedding_model"],
                    
                    # Content analysis
                    "contains_crisis_keywords": self._contains_crisis_keywords(doc["content"]),
                    "contains_student_keywords": self._contains_student_keywords(doc["content"]),
                    "priority_level": self._assess_content_priority(doc["content"], doc["content_type"]),
                    
                    # Searchable tags
                    "tags": self._extract_content_tags(doc["content"], doc["content_type"])
                }
                
                point = PointStruct(
                    id=str(uuid.uuid4()),
                    vector=doc["embedding"],
                    payload=payload
                )
                points.append(point)
            
            if not points:
                print("‚ùå Kh√¥ng c√≥ points h·ª£p l·ªá ƒë·ªÉ th√™m")
                return False
            
            # Batch upload v·ªõi progress tracking
            batch_size = 100
            total_batches = (len(points) + batch_size - 1) // batch_size
            
            for i in range(0, len(points), batch_size):
                batch = points[i:i + batch_size]
                batch_num = i // batch_size + 1
                
                print(f"   Batch {batch_num}/{total_batches}: {len(batch)} points")
                
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=batch
                )
            
            print(f"‚úÖ ƒê√£ th√™m {len(points)} documents th√†nh c√¥ng")
            
            # Verify upload
            collection_info = self.get_collection_info()
            print(f"üìä Collection hi·ªán c√≥: {collection_info.get('vectors_count', 0)} vectors")
            
            return True
            
        except Exception as e:
            print(f"‚ùå L·ªói th√™m documents: {e}")
            return False
    
    def _contains_crisis_keywords(self, content: str) -> bool:
        """Ki·ªÉm tra n·ªôi dung c√≥ ch·ª©a t·ª´ kh√≥a kh·ªßng ho·∫£ng kh√¥ng"""
        content_lower = content.lower()
        return any(keyword in content_lower for keyword in MENTAL_HEALTH_KEYWORDS["crisis_indicators"])
    
    def _contains_student_keywords(self, content: str) -> bool:
        """Ki·ªÉm tra n·ªôi dung c√≥ li√™n quan ƒë·∫øn h·ªçc sinh sinh vi√™n kh√¥ng"""
        content_lower = content.lower()
        return any(keyword in content_lower for keyword in MENTAL_HEALTH_KEYWORDS["student_specific"])
    
    def _assess_content_priority(self, content: str, content_type: str) -> str:
        """ƒê√°nh gi√° m·ª©c ƒë·ªô ∆∞u ti√™n c·ªßa n·ªôi dung"""
        if content_type == "crisis_support":
            return "critical"
        elif content_type == "intervention_guidance":
            return "high"
        elif content_type == "student_focused":
            return "medium"
        else:
            return "normal"
    
    def _extract_content_tags(self, content: str, content_type: str) -> List[str]:
        """Tr√≠ch xu·∫•t tags t·ª´ n·ªôi dung"""
        tags = [content_type]
        content_lower = content.lower()
        
        # Th√™m tags d·ª±a tr√™n keywords
        for category, keywords in MENTAL_HEALTH_KEYWORDS.items():
            for keyword in keywords:
                if keyword.lower() in content_lower:
                    tags.append(f"{category}:{keyword}")
        
        return list(set(tags))  # Remove duplicates
    
    def search_similar_documents(self, query_embedding: List[float], 
                               limit: int = None, 
                               filter_conditions: Dict = None,
                               score_threshold: float = None) -> List[Dict]:
        """
        T√¨m ki·∫øm documents t∆∞∆°ng ƒë·ªìng v·ªõi query embedding
        """
        if limit is None:
            limit = Config.TOP_K_DOCUMENTS
        if score_threshold is None:
            score_threshold = Config.SIMILARITY_THRESHOLD
        
        try:
            print(f"üîç T√¨m ki·∫øm {limit} documents t∆∞∆°ng ƒë·ªìng nh·∫•t...")
            print(f"   Score threshold: {score_threshold}")
            
            # T·∫°o filter n·∫øu c√≥
            search_filter = None
            if filter_conditions:
                conditions = []
                for key, value in filter_conditions.items():
                    if isinstance(value, list):
                        for v in value:
                            conditions.append(FieldCondition(key=key, match=MatchValue(value=v)))
                    else:
                        conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))
                
                search_filter = Filter(must=conditions)
            
            # Th·ª±c hi·ªán search
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=limit,
                score_threshold=score_threshold,
                query_filter=search_filter,
                with_payload=True,
                with_vectors=False  # Kh√¥ng c·∫ßn return vectors ƒë·ªÉ ti·∫øt ki·ªám bandwidth
            )
            
            if not search_results:
                print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y documents ph√π h·ª£p")
                return []
            
            # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£
            results = []
            for result in search_results:
                doc = {
                    "id": result.id,
                    "score": result.score,
                    "content": result.payload["content"],
                    "source": result.payload["source"],
                    "chunk_id": result.payload["chunk_id"],
                    "content_type": result.payload["content_type"],
                    "priority_level": result.payload.get("priority_level", "normal"),
                    "contains_crisis_keywords": result.payload.get("contains_crisis_keywords", False),
                    "contains_student_keywords": result.payload.get("contains_student_keywords", False),
                    "tags": result.payload.get("tags", [])
                }
                results.append(doc)
            
            print(f"‚úÖ T√¨m th·∫•y {len(results)} documents")
            
            # Log priority content
            crisis_docs = [d for d in results if d["contains_crisis_keywords"]]
            if crisis_docs:
                print(f"‚ö†Ô∏è  T√¨m th·∫•y {len(crisis_docs)} documents c√≥ n·ªôi dung kh·ªßng ho·∫£ng")
            
            return results
            
        except Exception as e:
            print(f"‚ùå L·ªói t√¨m ki·∫øm: {e}")
            return []
    
    def get_collection_info(self) -> Dict:
        """
        L·∫•y th√¥ng tin collection
        """
        try:
            collection_info = self.client.get_collection(self.collection_name)
            
            # Convert to dict safely
            if hasattr(collection_info, 'model_dump'):
                info_dict = collection_info.model_dump()
            elif hasattr(collection_info, 'dict'):
                info_dict = collection_info.dict()
            else:
                info_dict = vars(collection_info)
            
            # Extract key information
            result = {
                "collection_name": self.collection_name,
                "status": info_dict.get("status", "unknown"),
                "vectors_count": info_dict.get("vectors_count", 0),
                "indexed_vectors_count": info_dict.get("indexed_vectors_count", 0),
                "points_count": info_dict.get("points_count", 0),
                "config": info_dict.get("config", {})
            }
            
            return result
            
        except Exception as e:
            return {
                "error": f"L·ªói l·∫•y th√¥ng tin collection: {e}",
                "collection_name": self.collection_name
            }
    
    def delete_collection(self):
        """
        X√≥a collection
        """
        try:
            print(f"üóëÔ∏è  X√≥a collection: {self.collection_name}")
            self.client.delete_collection(self.collection_name)
            print(f"‚úÖ ƒê√£ x√≥a collection th√†nh c√¥ng")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói x√≥a collection: {e}")
            return False
    
    def search_by_content_type(self, query_embedding: List[float], 
                              content_types: List[str], 
                              limit: int = None) -> List[Dict]:
        """
        T√¨m ki·∫øm documents theo lo·∫°i n·ªôi dung c·ª• th·ªÉ
        """
        filter_conditions = {"content_type": content_types}
        return self.search_similar_documents(
            query_embedding=query_embedding,
            limit=limit,
            filter_conditions=filter_conditions
        )
    
    def search_crisis_content(self, query_embedding: List[float], 
                             limit: int = 5) -> List[Dict]:
        """
        T√¨m ki·∫øm n·ªôi dung h·ªó tr·ª£ kh·ªßng ho·∫£ng
        """
        filter_conditions = {"contains_crisis_keywords": True}
        return self.search_similar_documents(
            query_embedding=query_embedding,
            limit=limit,
            filter_conditions=filter_conditions
        )
    
    def get_collection_stats(self) -> Dict:
        """
        L·∫•y th·ªëng k√™ chi ti·∫øt v·ªÅ collection
        """
        try:
            info = self.get_collection_info()
            
            if "error" in info:
                return info
            
            # L·∫•y th·ªëng k√™ theo content type
            content_type_stats = {}
            try:
                # Scroll through all points to get stats (for small collections)
                points, _ = self.client.scroll(
                    collection_name=self.collection_name,
                    limit=10000,  # Adjust based on collection size
                    with_payload=True,
                    with_vectors=False
                )
                
                for point in points:
                    content_type = point.payload.get("content_type", "unknown")
                    content_type_stats[content_type] = content_type_stats.get(content_type, 0) + 1
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ l·∫•y content type stats: {e}")
            
            stats = {
                **info,
                "content_type_distribution": content_type_stats,
                "total_content_types": len(content_type_stats)
            }
            
            return stats
            
        except Exception as e:
            return {"error": f"L·ªói l·∫•y stats: {e}"}
    
    def health_check(self) -> Dict:
        """
        Ki·ªÉm tra s·ª©c kh·ªèe c·ªßa Qdrant connection
        """
        try:
            # Test basic connection
            collections = self.client.get_collections()
            
            # Test collection access
            info = self.get_collection_info()
            
            return {
                "status": "healthy",
                "qdrant_url": Config.QDRANT_URL,
                "collection_name": self.collection_name,
                "collections_available": len(collections.collections),
                "collection_status": info.get("status", "unknown"),
                "vectors_count": info.get("vectors_count", 0)
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "qdrant_url": Config.QDRANT_URL,
                "collection_name": self.collection_name
            }
