#!/usr/bin/env python3
"""
Data Ingestion Pipeline cho Mental Health RAG Agent
X·ª≠ l√Ω c√°c t√†i li·ªáu PDF v·ªÅ t∆∞ v·∫•n t√¢m l√Ω h·ªçc sinh sinh vi√™n
"""

import os
import sys
import argparse
from pathlib import Path
from typing import List, Dict
import glob
from datetime import datetime

from utils.pdf_processor import PDFProcessor
from utils.embedding_manager import EmbeddingManager
from utils.qdrant_manager import QdrantManager
from config import Config, MENTAL_HEALTH_KEYWORDS

class MentalHealthDataIngestion:
    def __init__(self):
        """
        Kh·ªüi t·∫°o pipeline n·∫°p d·ªØ li·ªáu cho domain t√¢m l√Ω
        """
        print("üß† Kh·ªüi t·∫°o Mental Health Data Ingestion Pipeline...")
        print(f"   Domain: {Config.DOMAIN}")
        print(f"   Collection: {Config.COLLECTION_NAME}")
        
        # Kh·ªüi t·∫°o c√°c components
        self.pdf_processor = PDFProcessor()
        self.embedding_manager = EmbeddingManager()
        self.qdrant_manager = QdrantManager()
        
        print("‚úÖ Pipeline ƒë√£ s·∫µn s√†ng!")
    
    def check_prerequisites(self):
        """
        Ki·ªÉm tra c√°c ƒëi·ªÅu ki·ªán ti√™n quy·∫øt
        """
        print("üîç Ki·ªÉm tra ƒëi·ªÅu ki·ªán ti√™n quy·∫øt...")
        
        # Ki·ªÉm tra Qdrant connection
        try:
            qdrant_health = self.qdrant_manager.health_check()
            if qdrant_health["status"] == "healthy":
                print(f"‚úÖ K·∫øt n·ªëi Qdrant th√†nh c√¥ng")
                print(f"   Collections: {qdrant_health['collections_available']}")
            else:
                print(f"‚ùå L·ªói k·∫øt n·ªëi Qdrant: {qdrant_health.get('error')}")
                print("üí° H√£y kh·ªüi ƒë·ªông Qdrant server:")
                print("   docker run -p 6333:6333 qdrant/qdrant")
                return False
        except Exception as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi Qdrant: {e}")
            return False
        
        # Ki·ªÉm tra embedding model
        try:
            test_result = self.embedding_manager.test_embedding_quality()
            if test_result["status"] == "success":
                print(f"‚úÖ Embedding model ho·∫°t ƒë·ªông (dimension: {test_result['embedding_dimension']})")
            else:
                print(f"‚ùå L·ªói embedding model: {test_result.get('error')}")
                return False
        except Exception as e:
            print(f"‚ùå L·ªói embedding model: {e}")
            return False
        
        return True
    
    def find_pdf_files(self, paths: List[str]) -> List[str]:
        """
        T√¨m t·∫•t c·∫£ file PDF t·ª´ paths (c√≥ th·ªÉ l√† file ho·∫∑c folder)
        """
        pdf_files = []
        
        if not paths:
            # M·∫∑c ƒë·ªãnh t√¨m trong th∆∞ m·ª•c data
            paths = ["data"]
        
        for path in paths:
            path = Path(path)
            
            if path.is_file() and path.suffix.lower() == '.pdf':
                pdf_files.append(str(path))
                print(f"‚úÖ T√¨m th·∫•y file: {path}")
            elif path.is_dir():
                # T√¨m t·∫•t c·∫£ PDF trong folder
                pattern = str(path / "**" / "*.pdf")
                found_files = glob.glob(pattern, recursive=True)
                if found_files:
                    pdf_files.extend(found_files)
                    print(f"‚úÖ T√¨m th·∫•y {len(found_files)} PDF files trong {path}")
                    for f in found_files:
                        print(f"   - {Path(f).name}")
                else:
                    print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y PDF n√†o trong: {path}")
            else:
                print(f"‚ö†Ô∏è  ƒê∆∞·ªùng d·∫´n kh√¥ng t·ªìn t·∫°i: {path}")
        
        return pdf_files
    
    def analyze_pdf_content(self, pdf_files: List[str]) -> Dict:
        """
        Ph√¢n t√≠ch s∆° b·ªô n·ªôi dung PDF ƒë·ªÉ ƒë√°nh gi√° quality
        """
        print(f"\nüìä Ph√¢n t√≠ch n·ªôi dung {len(pdf_files)} PDF files...")
        
        analysis = {
            "total_files": len(pdf_files),
            "successfully_analyzed": 0,
            "mental_health_relevant": 0,
            "student_focused": 0,
            "crisis_support": 0,
            "files_analysis": []
        }
        
        for pdf_file in pdf_files:
            try:
                print(f"\nüîç Ph√¢n t√≠ch: {Path(pdf_file).name}")
                
                # Tr√≠ch xu·∫•t text m·∫´u ƒë·ªÉ ph√¢n t√≠ch
                text_sample = self.pdf_processor.extract_text_from_pdf(pdf_file)
                
                if not text_sample:
                    print("   ‚ùå Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c text")
                    continue
                
                sample_text = text_sample[:2000].lower()  # L·∫•y 2000 k√Ω t·ª± ƒë·∫ßu
                
                # Ph√¢n t√≠ch keywords
                mental_health_score = sum(1 for keyword in MENTAL_HEALTH_KEYWORDS["psychological_conditions"] if keyword.lower() in sample_text)
                student_score = sum(1 for keyword in MENTAL_HEALTH_KEYWORDS["student_specific"] if keyword.lower() in sample_text)
                crisis_score = sum(1 for keyword in MENTAL_HEALTH_KEYWORDS["crisis_indicators"] if keyword.lower() in sample_text)
                intervention_score = sum(1 for keyword in MENTAL_HEALTH_KEYWORDS["interventions"] if keyword.lower() in sample_text)
                
                file_analysis = {
                    "file": Path(pdf_file).name,
                    "text_length": len(text_sample),
                    "mental_health_keywords": mental_health_score,
                    "student_keywords": student_score,
                    "crisis_keywords": crisis_score,
                    "intervention_keywords": intervention_score,
                    "relevance_score": mental_health_score + student_score + intervention_score,
                    "is_relevant": (mental_health_score + student_score + intervention_score) > 2
                }
                
                analysis["files_analysis"].append(file_analysis)
                analysis["successfully_analyzed"] += 1
                
                if file_analysis["is_relevant"]:
                    analysis["mental_health_relevant"] += 1
                    print(f"   ‚úÖ Relevant (score: {file_analysis['relevance_score']})")
                
                if student_score > 0:
                    analysis["student_focused"] += 1
                
                if crisis_score > 0:
                    analysis["crisis_support"] += 1
                
            except Exception as e:
                print(f"   ‚ùå L·ªói ph√¢n t√≠ch: {e}")
                continue
        
        print(f"\nüìä K·∫øt qu·∫£ ph√¢n t√≠ch:")
        print(f"   - Files analyzed: {analysis['successfully_analyzed']}/{analysis['total_files']}")
        print(f"   - Mental health relevant: {analysis['mental_health_relevant']}")
        print(f"   - Student focused: {analysis['student_focused']}")
        print(f"   - Crisis support: {analysis['crisis_support']}")
        
        return analysis
    
    def process_pdfs(self, pdf_files: List[str], force_reprocess: bool = False) -> List[dict]:
        """
        X·ª≠ l√Ω danh s√°ch PDF files
        """
        if not pdf_files:
            print("‚ùå Kh√¥ng c√≥ file PDF n√†o ƒë·ªÉ x·ª≠ l√Ω!")
            return []
        
        print(f"\nüìÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(pdf_files)} PDF files...")
        
        all_documents = []
        success_count = 0
        
        for i, pdf_file in enumerate(pdf_files, 1):
            try:
                print(f"\nüìñ [{i}/{len(pdf_files)}] X·ª≠ l√Ω: {Path(pdf_file).name}")
                
                # Ki·ªÉm tra file t·ªìn t·∫°i
                if not os.path.exists(pdf_file):
                    print(f"   ‚ùå File kh√¥ng t·ªìn t·∫°i: {pdf_file}")
                    continue
                
                # X·ª≠ l√Ω PDF
                documents = self.pdf_processor.process_pdf(pdf_file)
                
                if documents:
                    all_documents.extend(documents)
                    success_count += 1
                    print(f"   ‚úÖ T·∫°o ƒë∆∞·ª£c {len(documents)} chunks")
                    
                    # Th·ªëng k√™ content types
                    content_types = {}
                    for doc in documents:
                        ctype = doc["content_type"]
                        content_types[ctype] = content_types.get(ctype, 0) + 1
                    
                    print(f"   üìä Content types:")
                    for ctype, count in content_types.items():
                        print(f"      - {ctype}: {count}")
                else:
                    print(f"   ‚ö†Ô∏è  Kh√¥ng t·∫°o ƒë∆∞·ª£c chunk n√†o")
                    
            except Exception as e:
                print(f"   ‚ùå L·ªói x·ª≠ l√Ω {pdf_file}: {e}")
                continue
        
        print(f"\nüìä K·∫øt qu·∫£ x·ª≠ l√Ω:")
        print(f"   - Th√†nh c√¥ng: {success_count}/{len(pdf_files)} files")
        print(f"   - T·ªïng chunks: {len(all_documents)}")
        
        return all_documents
    
    def create_embeddings(self, documents: List[dict]) -> List[dict]:
        """
        T·∫°o embeddings cho documents
        """
        if not documents:
            return []
        
        print(f"\nüßÆ T·∫°o embeddings cho {len(documents)} documents...")
        
        try:
            documents_with_embeddings = self.embedding_manager.embed_documents(documents)
            
            if documents_with_embeddings:
                print(f"‚úÖ ƒê√£ t·∫°o embeddings th√†nh c√¥ng")
                
                # Th·ªëng k√™ embedding
                stats = self.embedding_manager.get_embedding_stats(documents_with_embeddings)
                print(f"üìä Embedding stats:")
                print(f"   - Model: {stats.get('model_name')}")
                print(f"   - Dimension: {stats.get('embedding_dimension')}")
                print(f"   - Mean magnitude: {stats.get('mean_magnitude', 0):.3f}")
            
            return documents_with_embeddings
            
        except Exception as e:
            print(f"‚ùå L·ªói t·∫°o embeddings: {e}")
            return []
    
    def store_in_vector_db(self, documents: List[dict], clear_existing: bool = False) -> bool:
        """
        L∆∞u documents v√†o vector database
        """
        if not documents:
            print("‚ùå Kh√¥ng c√≥ documents ƒë·ªÉ l∆∞u!")
            return False
        
        print(f"\nüíæ L∆∞u {len(documents)} documents v√†o Qdrant...")
        
        try:
            # X√≥a collection c≈© n·∫øu c·∫ßn
            if clear_existing:
                print("üóëÔ∏è  X√≥a collection c≈©...")
                self.qdrant_manager.delete_collection()
            
            # T·∫°o collection
            vector_size = self.embedding_manager.embedding_dimension
            self.qdrant_manager.create_collection(vector_size, force_recreate=clear_existing)
            
            # L∆∞u documents
            success = self.qdrant_manager.add_documents(documents)
            
            if success:
                # Ki·ªÉm tra k·∫øt qu·∫£
                stats = self.qdrant_manager.get_collection_stats()
                print(f"‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng!")
                print(f"   Collection: {stats.get('collection_name')}")
                print(f"   Vectors: {stats.get('vectors_count', 0)}")
                print(f"   Content types: {stats.get('total_content_types', 0)}")
                
                # In ph√¢n b·ªë content types
                if "content_type_distribution" in stats:
                    print(f"   üìä Content type distribution:")
                    for ctype, count in stats["content_type_distribution"].items():
                        print(f"      - {ctype}: {count}")
            
            return success
            
        except Exception as e:
            print(f"‚ùå L·ªói l∆∞u v√†o Qdrant: {e}")
            return False
    
    def run_ingestion(self, pdf_paths: List[str] = None, clear_existing: bool = False, 
                     force_reprocess: bool = False, analyze_only: bool = False):
        """
        Ch·∫°y to√†n b·ªô pipeline ingestion
        """
        print("üß† B·∫ÆT ƒê·∫¶U MENTAL HEALTH DATA INGESTION PIPELINE")
        print("=" * 60)
        print(f"‚è∞ Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # B∆∞·ªõc 1: Ki·ªÉm tra prerequisites
        if not self.check_prerequisites():
            print("‚ùå Kh√¥ng ƒë√°p ·ª©ng ƒëi·ªÅu ki·ªán ti√™n quy·∫øt!")
            return False
        
        # B∆∞·ªõc 2: T√¨m PDF files
        print(f"\nüìÅ T√¨m PDF files...")
        if pdf_paths is None:
            pdf_paths = ["data"]  # M·∫∑c ƒë·ªãnh
        
        pdf_files = self.find_pdf_files(pdf_paths)
        
        if not pdf_files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y PDF files n√†o!")
            print("üí° H√£y ƒë·∫∑t PDF files v√†o th∆∞ m·ª•c data/")
            return False
        
        print(f"\n‚úÖ S·∫Ω x·ª≠ l√Ω {len(pdf_files)} PDF files")
        
        # B∆∞·ªõc 3: Ph√¢n t√≠ch n·ªôi dung
        analysis = self.analyze_pdf_content(pdf_files)
        
        if analyze_only:
            print(f"\nüìä PH√ÇN T√çCH HO√ÄN T·∫§T!")
            return True
        
        if analysis["mental_health_relevant"] == 0:
            print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y PDF n√†o c√≥ n·ªôi dung t√¢m l√Ω ph√π h·ª£p!")
            print("üí° H√£y ki·ªÉm tra l·∫°i n·ªôi dung c√°c file PDF")
            return False
        
        # B∆∞·ªõc 4: X·ª≠ l√Ω PDFs
        documents = self.process_pdfs(pdf_files, force_reprocess)
        
        if not documents:
            print("‚ùå Kh√¥ng c√≥ documents ƒë·ªÉ x·ª≠ l√Ω!")
            return False
        
        # B∆∞·ªõc 5: T·∫°o embeddings
        documents_with_embeddings = self.create_embeddings(documents)
        
        if not documents_with_embeddings:
            print("‚ùå Kh√¥ng t·∫°o ƒë∆∞·ª£c embeddings!")
            return False
        
        # B∆∞·ªõc 6: L∆∞u v√†o vector DB
        success = self.store_in_vector_db(documents_with_embeddings, clear_existing)
        
        if success:
            print(f"\nüéâ HO√ÄN TH√ÄNH DATA INGESTION!")
            print(f"üìä Th·ªëng k√™ cu·ªëi:")
            print(f"   - PDF files: {len(pdf_files)}")
            print(f"   - Documents: {len(documents_with_embeddings)}")
            print(f"   - Collection: {Config.COLLECTION_NAME}")
            print(f"   - Embedding model: {Config.EMBEDDING_MODEL}")
            return True
        else:
            print(f"\n‚ùå DATA INGESTION TH·∫§T B·∫†I!")
            return False

def main():
    """
    Main function v·ªõi argument parsing
    """
    parser = argparse.ArgumentParser(
        description="Mental Health Data Ingestion Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª• s·ª≠ d·ª•ng:
  python ingest_data.py                    # N·∫°p t·∫•t c·∫£ PDF t·ª´ th∆∞ m·ª•c 'data'
  python ingest_data.py file.pdf          # N·∫°p file c·ª• th·ªÉ
  python ingest_data.py folder/           # N·∫°p t·ª´ folder c·ª• th·ªÉ
  python ingest_data.py --clear           # X√≥a collection c≈© v√† n·∫°p t·ª´ 'data'
  python ingest_data.py --analyze         # Ch·ªâ ph√¢n t√≠ch n·ªôi dung, kh√¥ng n·∫°p
  python ingest_data.py --check           # Ki·ªÉm tra h·ªá th·ªëng
        """
    )
    
    parser.add_argument(
        "paths", 
        nargs='*',  # 0 ho·∫∑c nhi·ªÅu arguments
        help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn PDF files ho·∫∑c folders (m·∫∑c ƒë·ªãnh: th∆∞ m·ª•c 'data')"
    )
    
    parser.add_argument(
        "--clear", 
        action="store_true", 
        help="X√≥a collection c≈© tr∆∞·ªõc khi th√™m d·ªØ li·ªáu m·ªõi"
    )
    
    parser.add_argument(
        "--force", 
        action="store_true", 
        help="Force reprocess t·∫•t c·∫£ PDFs"
    )
    
    parser.add_argument(
        "--analyze", 
        action="store_true", 
        help="Ch·ªâ ph√¢n t√≠ch n·ªôi dung PDF, kh√¥ng n·∫°p v√†o database"
    )
    
    parser.add_argument(
        "--check", 
        action="store_true", 
        help="Ch·ªâ ki·ªÉm tra prerequisites v√† exit"
    )
    
    args = parser.parse_args()
    
    # Kh·ªüi t·∫°o pipeline
    try:
        pipeline = MentalHealthDataIngestion()
    except Exception as e:
        print(f"‚ùå L·ªói kh·ªüi t·∫°o pipeline: {e}")
        sys.exit(1)
    
    # N·∫øu ch·ªâ check
    if args.check:
        success = pipeline.check_prerequisites()
        if success:
            try:
                stats = pipeline.qdrant_manager.get_collection_stats()
                print(f"\nüìä Tr·∫°ng th√°i hi·ªán t·∫°i:")
                print(f"   Collection: {Config.COLLECTION_NAME}")
                print(f"   Vectors: {stats.get('vectors_count', 0)}")
                print(f"   Content types: {stats.get('total_content_types', 0)}")
            except:
                print(f"\nüìä Collection ch∆∞a t·ªìn t·∫°i")
        sys.exit(0 if success else 1)
    
    # X·ª≠ l√Ω paths - n·∫øu kh√¥ng c√≥ paths th√¨ d√πng th∆∞ m·ª•c data m·∫∑c ƒë·ªãnh
    pdf_paths = args.paths if args.paths else None
    
    # Ch·∫°y ingestion
    success = pipeline.run_ingestion(
        pdf_paths=pdf_paths,
        clear_existing=args.clear,
        force_reprocess=args.force,
        analyze_only=args.analyze
    )
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
