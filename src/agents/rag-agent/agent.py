from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams
from langchain_qdrant import QdrantVectorStore
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai.chat_models import ChatGoogleGenerativeAIError
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.messages import BaseMessage
from langchain.schema import HumanMessage, AIMessage
from langchain_core.documents import Document
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from typing import Dict, List, Optional, Any, TypedDict
from pydantic import BaseModel
from config import Config
from datetime import datetime
import logging

logger = logging.getLogger(__name__)




class RAGState(TypedDict):
    # Input cá»§a ngÆ°á»i dÃ¹ng
    query: str
    user_context: Dict[str, Any]

    # Xá»­ lÃ½ input vÃ  tÃ¬m document liÃªn quan
    query_embedding: Optional[List[float]]
    retrieved_documents: List[Dict[str, Any]]
    relevant_documents: List[Dict[str, Any]]
    
    context: str
    
    # Output
    answer: str
    sources: List[str]
    
    #Metadata
    messages: List[BaseMessage]
    step: str
    processing_time: float
    status: str
    error: Optional[str]

class RAGAgent:
    def __init__(self):
        """
            Khá»Ÿi táº¡o RAG Agent
        """
        print(f"Khá»Ÿi táº¡o RAG Agent vá»›i LLM model: {Config.GOOGLE_LLM_MODEL} ...")
        print(f"Khá»Ÿi táº¡o model vá»›i API Key ...")
        
        # Kiá»ƒm tra API Key
        self.google_api_key = str(Config.GOOGLE_API_KEY)
        if not self.google_api_key:
            raise ValueError("GEMINI_API_KEY environment variable is required")
        elif self.google_api_key == "your_google_api_key":
            raise ValueError("Set your GEMINI_API_KEY variable in .env file")
        
        # Khá»Ÿi táº¡o LLM vá»›i Google API Key
        try:
            self.llm = ChatGoogleGenerativeAI(
                model=Config.GOOGLE_LLM_MODEL,
                temperature=Config.GOOGLE_LLM_TEMPERATURE,
                max_output_tokens=Config.GOOGLE_LLM_MAX_OUTPUT_TOKENS,
                google_api_key=Config.GOOGLE_API_KEY

            )
            logger.info(f"Khá»Ÿi táº¡o model LLM {Config.GOOGLE_LLM_MODEL} thÃ nh cÃ´ng")
        except ChatGoogleGenerativeAIError as e:
            logger.error(f"Lá»—i khi khá»Ÿi táº¡o model LLM {Config.GOOGLE_LLM_MODEL}: {e}")
            exit(1)
        
        # Khá»Ÿi táº¡o embedding model HuggingFace
        self._init_embedding()
        
        # Káº¿t ná»‘i tá»›i Qdrant DB
        self.qdrant_client = QdrantClient(url=Config.QDRANT_URL)
        logger.info(f"Káº¿t ná»‘i tá»›i Qdrant DB thÃ nh cÃ´ng")

        # Kiá»ƒm tra collection Ä‘Ã£ cÃ³ chÆ°a, náº¿u khÃ´ng thÃ¬ táº¡o má»›i
        if not self.qdrant_client.collection_exists(Config.COLLECTION_NAME):
            logger.warning(f"Collection {Config.COLLECTION_NAME} is not existed, creating ...")
            self.qdrant_client.create_collection(
                collection_name=Config.COLLECTION_NAME,
                vectors_config=VectorParams(size=768, distance=Distance.COSINE)
            )
            logger.info(f"Collection {Config.COLLECTION_NAME} created")
        # Khá»Ÿi táº¡o vector store
        self.vector_store = QdrantVectorStore(
            embedding=self.embeddings,
            client=self.qdrant_client, 
            collection_name=Config.COLLECTION_NAME
        )
        
        # Táº¡o workflow
        self.workflow = self._create_workflow()
        self.compiled_workflow = self.workflow.compile()
    def _init_embedding(self):
        model_kwargs = { "device": "cpu", "trust_remote_code": True }
        encode_kwargs = { 'normalize_embeddings': True, 'batch_size': 16 }
        try:
                
            self.embeddings = HuggingFaceEmbeddings(
                model_name=Config.EMBEDDING_MODEL,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs,
                show_progress=True
            )
            logger.info(f"Khá»Ÿi táº¡o Embedding Model {Config.EMBEDDING_MODEL} thÃ nh cÃ´ng")
        except Exception as e:
            logger.error(f"KhÃ´ng thá»ƒ khá»Ÿi táº¡o Embedding Model {Config.EMBEDDING_MODEL}")
            exit(1)

    def _create_workflow(self) -> StateGraph:
        workflow = StateGraph(RAGState)

        workflow.add_node("retrieve_documents", self.retrieve_documents_node)
        workflow.add_node("filter_documents", self.filter_documents_node)
        workflow.add_node("aggregate_context", self.aggregate_context_node)
        workflow.add_node("generate_answer", self.generate_answer_node)
        workflow.add_node("error_handle", self.error_handle_node)

        # Set entry point
        workflow.set_entry_point("retrieve_documents")
        
        workflow.add_edge("retrieve_documents", "filter_documents")
        workflow.add_edge("filter_documents", "aggregate_context")
        workflow.add_edge("aggregate_context", "generate_answer")
        workflow.add_edge("generate_answer", END)
        workflow.add_edge("error_handle", END)

        return workflow
    
    def retrieve_documents_node(self, state: RAGState) -> RAGState:
        
        try:
            state["step"] = "Truy xuáº¥t tÃ i liá»‡u"
            state["messages"] = add_messages(
                state.get("messages", []), 
                [HumanMessage(content="Äang tÃ¬m kiáº¿m thÃ´ng tin liÃªn quan...")])
            
            # Kiá»ƒm tra vector store vÃ  embedding model
            if not self.qdrant_client or not self.embeddings:
                raise Exception("Qdrant Client hoáº·c Embedding Manager chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p")
            
            # Táº¡o embed cho query cá»§a ngÆ°á»i dÃ¹ng
            embedded_query = self.embeddings.embed_query(state["query"])
            search_results = self.qdrant_client.search(
                collection_name=Config.COLLECTION_NAME,
                query_vector=embedded_query,
                limit=Config.TOP_K_DOCUMENTS,  # Giá»›i háº¡n sá»‘ lÆ°á»£ng results
                score_threshold=Config.SIMILARITY_THRESHOLD,
                with_payload=True,
                with_vectors=False
            )

            retrieved_documents = []
            
            for hit in search_results:
                result_dict = {
                    "id": str(hit.id),
                    "score": hit.score,
                    "content": hit.payload.get("content", ""),
                    "source": hit.payload.get("source", ""),
                    "chunk_index": hit.payload.get("chunk_index", 0),
                    "doc_id": hit.payload.get("doc_id", ""),
                    "section": hit.payload.get("section", "")
                }
                retrieved_documents.append(result_dict)


            state["retrieved_documents"] = retrieved_documents
            state["status"] = "document_retrieved"
            return state
        except Exception as e:
            state["error"] = f"Lá»—i truy xuáº¥t tÃ i liá»‡u: {str(e)}"
            state["status"] = "error"
            return state

    def filter_documents_node(self, state: RAGState) -> RAGState:
        try:
            state["step"] = "Lá»c document loáº¡i bá» tÃ i liá»‡u Ä‘iá»ƒm tháº¥p"
            state["messages"] = add_messages(
                state.get("messages", []), 
                [HumanMessage(content="Äang lá»c tÃ i liá»‡u ...")])
            # filter documents Ä‘Æ°á»£c retrieved, lÆ°u vÃ o filtered_docs
            relevant_documents = self.filter_documents(state["query"], state["retrieved_documents"])
            
            state["relevant_documents"] = relevant_documents
            state["status"] = "filtered_documents"
            return state
        except Exception as e:
            state["error"] = f"Lá»—i filter tÃ i liá»‡u: {str(e)}"
            state["status"] = "error"
            return state
        
    def aggregate_context_node(self, state: RAGState) -> RAGState:
        try:
            state["step"] = "Tá»•ng há»£p tÃ i liá»‡u"
            state["messages"] = add_messages(
                state.get("messages", []), 
                [HumanMessage(content="Äang tá»•ng há»£p thÃ´ng tin tá»« tÃ i liá»‡u...")]
            )
            # Tá»•ng há»£p context
            context = self.aggregate_context(state["relevant_documents"])
            state["context"] = context
            state["sources"] = list(set([doc["source"] for doc in state["relevant_documents"]]))
            state["status"] = "context_aggregated"

            
            return state
        
        except Exception as e:
            state["error"] = f"Lá»—i tá»•ng há»£p context tá»« tÃ i liá»‡u: {str(e)}"
            state["status"] = "error"
            return state
        
    def generate_answer_node(self, state: RAGState) -> RAGState:
        try:
            state["step"] = "Táº¡o cÃ¢u tráº£ lá»i, hoÃ n thÃ nh"
            state["messages"] = add_messages(
                state.get("messages", []), 
                [HumanMessage(content="Äang táº¡o cÃ¢u tráº£ lá»i ...")]
            )

            prompt = f"""
                Báº¡n lÃ  má»™t chuyÃªn gia tÆ° váº¥n tÃ¢m lÃ½ há»c Ä‘Æ°á»ng vÃ  sá»©c khá»e tinh tháº§n, cÃ³ nhiá»‡m vá»¥ há»— trá»£ há»c sinh, sinh viÃªn, giÃ¡o viÃªn vÃ  chuyÃªn gia tÃ¢m lÃ½. 
                Báº¡n cáº§n thá»ƒ hiá»‡n sá»± Ä‘á»“ng cáº£m, nháº¹ nhÃ ng vÃ  cÃ³ cÆ¡ sá»Ÿ khoa há»c khi pháº£n há»“i.

                Ngá»¯ cáº£nh (cÃ¡c tÃ i liá»‡u tham kháº£o tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u):
                {state["context"]}

                CÃ¢u há»i hoáº·c váº¥n Ä‘á» ngÆ°á»i dÃ¹ng Ä‘áº·t ra:
                {state["query"]}

                YÃªu cáº§u pháº£n há»“i:
                - Giáº£i thÃ­ch rÃµ rÃ ng, dá»… hiá»ƒu, trÃ¡nh ngÃ´n ngá»¯ há»c thuáº­t phá»©c táº¡p.
                - Cung cáº¥p hÆ°á»›ng dáº«n cá»¥ thá»ƒ Ä‘á»ƒ giÃºp há» hiá»ƒu, Ä‘á»‘i diá»‡n vÃ  cáº£i thiá»‡n váº¥n Ä‘á» sá»©c khá»e tinh tháº§n.
                - Thá»ƒ hiá»‡n sá»± láº¯ng nghe, khÃ­ch lá»‡ vÃ  Ä‘á»“ng cáº£m. 
                - Náº¿u cÃ¢u há»i cÃ³ dáº¥u hiá»‡u kháº©n cáº¥p (liÃªn quan Ä‘áº¿n tá»± háº¡i, tá»± tá»­, báº¡o lá»±c, khá»§ng hoáº£ng cáº£m xÃºc), hÃ£y Æ°u tiÃªn **an toÃ n**:
                    > "Náº¿u em Ä‘ang trong tÃ¬nh tráº¡ng khá»§ng hoáº£ng hoáº·c cÃ³ Ã½ Ä‘á»‹nh lÃ m háº¡i báº£n thÃ¢n, hÃ£y liÃªn há»‡ ngay vá»›i ngÆ°á»i thÃ¢n, báº¡n bÃ¨ hoáº·c chuyÃªn gia tÃ¢m lÃ½ táº¡i trÆ°á»ng. Em khÃ´ng Ä‘Æ¡n Ä‘á»™c vÃ  cÃ³ ngÆ°á»i sáºµn sÃ ng giÃºp Ä‘á»¡."

                3. Khi tráº£ lá»i, luÃ´n giá»¯ thÃ¡i Ä‘á»™ nhÃ¢n vÄƒn, tÃ´n trá»ng vÃ  mang tÃ­nh há»— trá»£. 
                - KhÃ´ng Ä‘Æ°a ra cháº©n Ä‘oÃ¡n y khoa hay káº¿t luáº­n bá»‡nh lÃ½.
                - Náº¿u thiáº¿u thÃ´ng tin, hÃ£y nÃ³i rÃµ ráº±ng cáº§n thÃªm dá»¯ liá»‡u hoáº·c nÃªn tham kháº£o chuyÃªn gia.

                Äá»‹nh dáº¡ng pháº£n há»“i:
                - Giáº£i thÃ­ch thÃ¢n thiá»‡n, rÃµ rÃ ng, cÃ³ thá»ƒ chia nhá» tá»«ng Ã½.
                - TrÃ¬nh bÃ y tá»± nhiÃªn, gáº§n gÅ©i vá»›i há»c sinh â€“ sinh viÃªn Viá»‡t Nam.
                - *LÆ¯U Ã*: KHÃ”NG trÃ­ch dáº«n nguá»“n hay viáº¿t báº¥t ká»³ thá»© gÃ¬ liÃªn quan tá»›i ná»™i dung trÃ­ch dáº«n nguá»“n nhÆ°: "Báº¡n cÃ³ thá»ƒ tham kháº£o tÃ i liá»‡u ..."
            """

            response = self.llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)

            state["answer"] = answer
            state["status"] = "completed"
            state["processing_time"] = 0
            state["messages"] = add_messages(
                state.get("messages", []), 
                [AIMessage(content=answer)])
            return state
        except Exception as e:
            state["error"] = f"Lá»—i táº¡o cÃ¢u tráº£ lá»i: {str(e)}"
            state["status"] = "error"
            return state
        
    def error_handle_node(self, state: RAGState) -> RAGState:
        message = f"Lá»—i khi xá»­ lÃ½ cÃ¢u há»i: {state.get('error', "Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh")}"
        state["messages"] = add_messages(
            state.get("messages", []), 
            [AIMessage(content=message)]
        )
        state["status"] = "error_handled"
        return state
        
    def invoke(self, query: str, user_context: Dict = None) -> Dict[str, Any]:
        init_state: RAGState = {
            "query": query,
            "user_context": user_context or {},
            "query_embedding": None,
            "retrieved_documents": [],
            "relevant_documents": [],
            "context": "",
            "answer": "",
            "sources": [],
            "messages": [],
            "step": "completed",
            "processing_time": 0,
            "status": "completed",
            "error": None
        }

        try:
            start_time = datetime.now()
            final_state = self.compiled_workflow.invoke(init_state)
            processing_time = (datetime.now() - start_time).total_seconds()

            return {
                "answer": final_state.get("answer", ""),
                "sources": final_state.get("sources", []),
                "relevant_documents_count": len(final_state.get("relevant_documents", [])),
                "total_retrieved_count": len(final_state.get("retrieved_documents", [])),
                "processing_time": processing_time,
                "status": final_state.get("status", "unknown")
            }
        
        except Exception as e:
            return {
                "answer": f"Lá»—i xá»­ lÃ½: {str(e)}",
                "sources": [],
                "relevant_documents_count": 0,
                "total_retrieved_count": 0,
                "processing_time": 0.0,
                "status": "error"
            }

    def filter_documents(self, query: str, docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        relevant_docs = []
        print(f"Filtering {len(docs)} documents with LLM grading...")
        
        for doc in docs:
            # Documents Ä‘Ã£ Ä‘Æ°á»£c filter theo score á»Ÿ Qdrant level
            check_prompt = f"""
            HÃ£y Ä‘Ã¡nh giÃ¡ xem Ä‘oáº¡n vÄƒn báº£n sau cÃ³ liÃªn quan tá»›i cÃ¢u há»i khÃ´ng?

            CÃ¢u há»i: {query}

            Äoáº¡n vÄƒn báº£n: {doc["content"][:500]}...
            
            Chá»‰ tráº£ lá»i "YES" náº¿u liÃªn quan hoáº·c "NO" náº¿u khÃ´ng liÃªn quan.
            """
            
            try:
                messages = [HumanMessage(content=check_prompt)]
                response = self.llm.invoke(messages)
                grade = response.content.strip().upper()

                if grade == "YES":
                    print(f"âœ… Document tá»« {doc['source']}, chunk {doc['chunk_index']} Ä‘Æ°á»£c cháº¥p nháº­n (score: {doc['score']:.3f})")
                    relevant_docs.append(doc)
                elif grade == "NO":
                    print(f"âŒ Document tá»« {doc['source']}, chunk {doc['chunk_index']} khÃ´ng Ä‘Æ°á»£c cháº¥p nháº­n (score: {doc['score']:.3f})")
                else:
                    print(f"âš ï¸ LLM tráº£ lá»i khÃ´ng rÃµ rÃ ng: '{grade}', giá»¯ document dá»±a trÃªn score")
                    relevant_docs.append(doc)
                    
            except Exception as e:
                print(f"âš ï¸ LLM grading failed: {e}, giá»¯ document dá»±a trÃªn similarity score")
                relevant_docs.append(doc)
        
        print(f"ðŸ“Š Filtering complete: {len(relevant_docs)}/{len(docs)} documents passed LLM grading")
        return relevant_docs
    
    def aggregate_context(self, docs: List[Dict[str, Any]]) -> str:
        if not docs:
            return ""
        context_parts = []
        for doc in docs:
            source = f"Nguá»“n: {doc['source']}, "
            content = f"{source}\nNá»™i dung: {doc['content']}"
            context_parts.append(content)
        
        context = "\n" + ("="*50 + "\n").join(context_parts)
        return context
    
    def health_check(self) -> Dict[str, Any]:
        try:
            if not self.llm:
                logger.error("LLM model chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o")
                llm_status = "unhealthy"
            else:
                llm_status = "healthy"
            if not self.qdrant_client:
                logger.error("Qdrant Client chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o")
                qdrant_status = "unhealthy"
            else:
                qdrant_status = "healthy"
            if not self.embeddings:
                logger.error("Embedding model chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o")
                embedding_status = "unhealthy"
            else:
                embedding_status = "healthy"
            if not self.vector_store:
                logger.error("Vector store chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o")
                vector_store_status = "unhealthy"
            else:
                vector_store_status = "healthy"
            
            return {
                "status": "healthy",
                "components": {
                    "llm": {
                        "llm_model": Config.GOOGLE_LLM_MODEL,
                        "status": llm_status
                    },
                    "qdrant_client": {
                        "status": qdrant_status
                    },
                    "embedding": {
                        "embedding_model": Config.EMBEDDING_MODEL,
                        "status": embedding_status
                    },
                    "vector_store": {
                        "status": vector_store_status
                    }
                },
                "protocol": "A2A",
                "qdrant_url": Config.QDRANT_URL,
                "collection_name": Config.COLLECTION_NAME,
                "workflow_type": "LangGraph",
                "workflow_nodes": list(self.workflow.nodes.keys())
            }
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return {
                "status": "unhealthy",
                "error": str(e)
            }
        

           